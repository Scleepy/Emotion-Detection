{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32859209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bd37dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT DATASET\n",
    "dataset_dir = \"dataset/ISEAR.csv\"\n",
    "df = pd.read_csv(dataset_dir, encoding='latin-1')\n",
    "\n",
    "df = df[df[\"Text\"] != \"[ No response.]\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "289fe3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "max_embedding_length = 300 #MAX EMBEDDING DIMENSION\n",
    "max_sequence_length = 400 #INPUT LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "333f04ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d62b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "df['Text'] = df['Text'].apply(lambda x: re.sub('[0-9]+', '', x))\n",
    "\n",
    "# Remove punctuations\n",
    "df['Text'] = df['Text'].apply(lambda x: re.sub('[^\\w\\s]+', '', x))\n",
    "\n",
    "# Convert to lowercase\n",
    "df['Text'] = df['Text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Text'] = df['Text'].apply(lambda x: ' '.join(\n",
    "    [word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Tokenize\n",
    "df['Tokenized'] = df['Text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in text]\n",
    "    return lemmatized\n",
    "\n",
    "df['Lemmatized'] = df['Tokenized'].apply(lemmatize)\n",
    "\n",
    "\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "df['Detokenized'] = df['Lemmatized'].apply(lambda x: detokenizer.detokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afc6c8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>Lemmatized</th>\n",
       "      <th>Detokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>period falling love time met especially met lo...</td>\n",
       "      <td>[period, falling, love, time, met, especially,...</td>\n",
       "      <td>[period, falling, love, time, met, especially,...</td>\n",
       "      <td>period falling love time met especially met lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>involved traffic accident</td>\n",
       "      <td>[involved, traffic, accident]</td>\n",
       "      <td>[involved, traffic, accident]</td>\n",
       "      <td>involved traffic accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>driving home several days hard work motorist a...</td>\n",
       "      <td>[driving, home, several, days, hard, work, mot...</td>\n",
       "      <td>[driving, home, several, day, hard, work, moto...</td>\n",
       "      <td>driving home several day hard work motorist ah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>lost person meant</td>\n",
       "      <td>[lost, person, meant]</td>\n",
       "      <td>[lost, person, meant]</td>\n",
       "      <td>lost person meant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>time knocked deer sight animals injuries helpl...</td>\n",
       "      <td>[time, knocked, deer, sight, animals, injuries...</td>\n",
       "      <td>[time, knocked, deer, sight, animal, injury, h...</td>\n",
       "      <td>time knocked deer sight animal injury helpless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7661</th>\n",
       "      <td>anger</td>\n",
       "      <td>two years back someone invited tutor granddaug...</td>\n",
       "      <td>[two, years, back, someone, invited, tutor, gr...</td>\n",
       "      <td>[two, year, back, someone, invited, tutor, gra...</td>\n",
       "      <td>two year back someone invited tutor granddaugh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7662</th>\n",
       "      <td>sadness</td>\n",
       "      <td>taken responsibility something prepared howeve...</td>\n",
       "      <td>[taken, responsibility, something, prepared, h...</td>\n",
       "      <td>[taken, responsibility, something, prepared, h...</td>\n",
       "      <td>taken responsibility something prepared howeve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7663</th>\n",
       "      <td>disgust</td>\n",
       "      <td>home heard loud sound spitting outside door th...</td>\n",
       "      <td>[home, heard, loud, sound, spitting, outside, ...</td>\n",
       "      <td>[home, heard, loud, sound, spitting, outside, ...</td>\n",
       "      <td>home heard loud sound spitting outside door th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7664</th>\n",
       "      <td>shame</td>\n",
       "      <td>homework teacher asked us scolded immediately</td>\n",
       "      <td>[homework, teacher, asked, us, scolded, immedi...</td>\n",
       "      <td>[homework, teacher, asked, u, scolded, immedia...</td>\n",
       "      <td>homework teacher asked u scolded immediately</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7665</th>\n",
       "      <td>guilt</td>\n",
       "      <td>shouted younger brother always afraid called l...</td>\n",
       "      <td>[shouted, younger, brother, always, afraid, ca...</td>\n",
       "      <td>[shouted, younger, brother, always, afraid, ca...</td>\n",
       "      <td>shouted younger brother always afraid called l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7589 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Emotion                                               Text   \n",
       "0         joy  period falling love time met especially met lo...  \\\n",
       "1        fear                          involved traffic accident   \n",
       "2       anger  driving home several days hard work motorist a...   \n",
       "3     sadness                                  lost person meant   \n",
       "4     disgust  time knocked deer sight animals injuries helpl...   \n",
       "...       ...                                                ...   \n",
       "7661    anger  two years back someone invited tutor granddaug...   \n",
       "7662  sadness  taken responsibility something prepared howeve...   \n",
       "7663  disgust  home heard loud sound spitting outside door th...   \n",
       "7664    shame      homework teacher asked us scolded immediately   \n",
       "7665    guilt  shouted younger brother always afraid called l...   \n",
       "\n",
       "                                              Tokenized   \n",
       "0     [period, falling, love, time, met, especially,...  \\\n",
       "1                         [involved, traffic, accident]   \n",
       "2     [driving, home, several, days, hard, work, mot...   \n",
       "3                                 [lost, person, meant]   \n",
       "4     [time, knocked, deer, sight, animals, injuries...   \n",
       "...                                                 ...   \n",
       "7661  [two, years, back, someone, invited, tutor, gr...   \n",
       "7662  [taken, responsibility, something, prepared, h...   \n",
       "7663  [home, heard, loud, sound, spitting, outside, ...   \n",
       "7664  [homework, teacher, asked, us, scolded, immedi...   \n",
       "7665  [shouted, younger, brother, always, afraid, ca...   \n",
       "\n",
       "                                             Lemmatized   \n",
       "0     [period, falling, love, time, met, especially,...  \\\n",
       "1                         [involved, traffic, accident]   \n",
       "2     [driving, home, several, day, hard, work, moto...   \n",
       "3                                 [lost, person, meant]   \n",
       "4     [time, knocked, deer, sight, animal, injury, h...   \n",
       "...                                                 ...   \n",
       "7661  [two, year, back, someone, invited, tutor, gra...   \n",
       "7662  [taken, responsibility, something, prepared, h...   \n",
       "7663  [home, heard, loud, sound, spitting, outside, ...   \n",
       "7664  [homework, teacher, asked, u, scolded, immedia...   \n",
       "7665  [shouted, younger, brother, always, afraid, ca...   \n",
       "\n",
       "                                            Detokenized  \n",
       "0     period falling love time met especially met lo...  \n",
       "1                             involved traffic accident  \n",
       "2     driving home several day hard work motorist ah...  \n",
       "3                                     lost person meant  \n",
       "4     time knocked deer sight animal injury helpless...  \n",
       "...                                                 ...  \n",
       "7661  two year back someone invited tutor granddaugh...  \n",
       "7662  taken responsibility something prepared howeve...  \n",
       "7663  home heard loud sound spitting outside door th...  \n",
       "7664       homework teacher asked u scolded immediately  \n",
       "7665  shouted younger brother always afraid called l...  \n",
       "\n",
       "[7589 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "121f2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06ce8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Emotion'])\n",
    "\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b82bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "010fb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Detokenized'], y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e0f7015",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_arr = [''.join(text) for text in df['Detokenized']]\n",
    "\n",
    "text_train_arr = [''.join(text) for text in X_train]\n",
    "text_test_arr = [''.join(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61e80242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e4e034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_arr)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(text_train_arr)\n",
    "sequences_test = tokenizer.texts_to_sequences(text_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d9d1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = pad_sequences(sequences_train, maxlen = max_sequence_length )\n",
    "X_test_padded = pad_sequences(sequences_test, maxlen = max_sequence_length )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32529c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09cbd789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMBEDDING MATRIX CREATION -> CHOOSE ONE\n",
    "\n",
    "#1ST CHOICE -> USING GLOVE OR FASTTEXT -> JUST CHANGE THE FILEPATH # wiki-news-300d-1M.vec # glove.6B.300d.txt\n",
    "# embeddings = {}\n",
    "# with open('embedding/wiki-news-300d-1M.vec', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         token = line.split()[0]\n",
    "#         embeddings[token] = np.array(line.split()[1:], dtype='float32')\n",
    "\n",
    "#2ND CHOICE -> USING WORD2VEC\n",
    "embeddings = KeyedVectors.load_word2vec_format('embedding/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9d7d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, max_embedding_length))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < len(tokenizer.word_index) + 1 and word in embeddings:\n",
    "        embedding_matrix[i] = embeddings[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5aa4b61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.07080078, -0.21386719,  0.15332031, ..., -0.21679688,\n",
       "        -0.01977539,  0.10644531],\n",
       "       [-0.06689453,  0.07958984, -0.08398438, ...,  0.02575684,\n",
       "         0.31640625, -0.16796875],\n",
       "       ...,\n",
       "       [ 0.19042969,  0.30859375, -0.08496094, ...,  0.17578125,\n",
       "        -0.09912109,  0.31054688],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.0546875 ,  0.13769531,  0.29296875, ...,  0.16699219,\n",
       "         0.10693359,  0.1953125 ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f262109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional, GRU\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23109ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(type=\"LSTM\"):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, \n",
    "                        output_dim=max_embedding_length, \n",
    "                        input_length = max_sequence_length, \n",
    "                        weights = [embedding_matrix], \n",
    "                        trainable=False))\n",
    "\n",
    "    if (type == \"LSTM\"):\n",
    "        model.add(LSTM(128))\n",
    "    elif (type == \"BiLSTM\"):\n",
    "        model.add(Bidirectional(LSTM(128)))\n",
    "    else:\n",
    "        model.add(GRU(128))\n",
    "\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(X_train_padded, y_train, validation_data=(X_test_padded,y_test), epochs=10, batch_size= 128)\n",
    "\n",
    "    prediction_results = model.predict(X_test_padded)\n",
    "    prediction_results = np.argmax(prediction_results, axis=1)\n",
    "\n",
    "    y_test_result = np.argmax(y_test, axis=1)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_score(y_test_result, prediction_results) * 100:.2f}%\")\n",
    "    print(f\"Recall Score: {recall_score(y_test_result, prediction_results, average='weighted') * 100:.2f}\")\n",
    "    print(f\"Precision: {precision_score(y_test_result, prediction_results, average='weighted') * 100:.2f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test_result, prediction_results, average='weighted') * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7b0ad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 400, 300)          2481900   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               219648    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,702,451\n",
      "Trainable params: 220,551\n",
      "Non-trainable params: 2,481,900\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 5s 72ms/step - loss: 1.7168 - accuracy: 0.3528 - val_loss: 1.4802 - val_accuracy: 0.4855\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 3s 65ms/step - loss: 1.3413 - accuracy: 0.5243 - val_loss: 1.3148 - val_accuracy: 0.5198\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 3s 61ms/step - loss: 1.1946 - accuracy: 0.5706 - val_loss: 1.2807 - val_accuracy: 0.5303\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 3s 62ms/step - loss: 1.1102 - accuracy: 0.6070 - val_loss: 1.2086 - val_accuracy: 0.5692\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 3s 64ms/step - loss: 1.0440 - accuracy: 0.6317 - val_loss: 1.2192 - val_accuracy: 0.5534\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 3s 63ms/step - loss: 0.9937 - accuracy: 0.6473 - val_loss: 1.2085 - val_accuracy: 0.5758\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 3s 68ms/step - loss: 0.9430 - accuracy: 0.6582 - val_loss: 1.1974 - val_accuracy: 0.5711\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 3s 63ms/step - loss: 0.9020 - accuracy: 0.6786 - val_loss: 1.2475 - val_accuracy: 0.5672\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 3s 66ms/step - loss: 0.8588 - accuracy: 0.6951 - val_loss: 1.2489 - val_accuracy: 0.5672\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 3s 63ms/step - loss: 0.8139 - accuracy: 0.7094 - val_loss: 1.2229 - val_accuracy: 0.5685\n",
      "48/48 [==============================] - 1s 13ms/step\n",
      "Accuracy: 56.85%\n",
      "Recall Score: 56.85\n",
      "Precision: 57.85\n",
      "F1 Score: 57.23\n"
     ]
    }
   ],
   "source": [
    "test_model(\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f668530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 400, 300)          2481900   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              439296    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,922,995\n",
      "Trainable params: 441,095\n",
      "Non-trainable params: 2,481,900\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 9s 136ms/step - loss: 1.7424 - accuracy: 0.3540 - val_loss: 1.5195 - val_accuracy: 0.4427\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 6s 120ms/step - loss: 1.3780 - accuracy: 0.5029 - val_loss: 1.3272 - val_accuracy: 0.5204\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 6s 120ms/step - loss: 1.2148 - accuracy: 0.5693 - val_loss: 1.2646 - val_accuracy: 0.5408\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 6s 120ms/step - loss: 1.1247 - accuracy: 0.6009 - val_loss: 1.2337 - val_accuracy: 0.5520\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 6s 130ms/step - loss: 1.0518 - accuracy: 0.6256 - val_loss: 1.2127 - val_accuracy: 0.5698\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 7s 136ms/step - loss: 1.0034 - accuracy: 0.6437 - val_loss: 1.1965 - val_accuracy: 0.5856\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 6s 126ms/step - loss: 0.9674 - accuracy: 0.6503 - val_loss: 1.2253 - val_accuracy: 0.5520\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 6s 128ms/step - loss: 0.9357 - accuracy: 0.6650 - val_loss: 1.2033 - val_accuracy: 0.5626\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 6s 123ms/step - loss: 0.8825 - accuracy: 0.6887 - val_loss: 1.2171 - val_accuracy: 0.5685\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 6s 135ms/step - loss: 0.8388 - accuracy: 0.7010 - val_loss: 1.2274 - val_accuracy: 0.5758\n",
      "48/48 [==============================] - 3s 30ms/step\n",
      "Accuracy: 57.58%\n",
      "Recall Score: 57.58\n",
      "Precision: 58.22\n",
      "F1 Score: 57.76\n"
     ]
    }
   ],
   "source": [
    "test_model(\"BiLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c56efa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 400, 300)          2481900   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 128)               165120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,647,923\n",
      "Trainable params: 166,023\n",
      "Non-trainable params: 2,481,900\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 5s 71ms/step - loss: 1.7540 - accuracy: 0.3383 - val_loss: 1.4908 - val_accuracy: 0.4572\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 3s 57ms/step - loss: 1.3437 - accuracy: 0.5123 - val_loss: 1.3029 - val_accuracy: 0.5323\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 3s 61ms/step - loss: 1.1620 - accuracy: 0.5895 - val_loss: 1.2393 - val_accuracy: 0.5455\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 3s 56ms/step - loss: 1.0769 - accuracy: 0.6195 - val_loss: 1.1849 - val_accuracy: 0.5718\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 3s 55ms/step - loss: 1.0169 - accuracy: 0.6408 - val_loss: 1.1660 - val_accuracy: 0.5823\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 3s 59ms/step - loss: 0.9688 - accuracy: 0.6552 - val_loss: 1.1879 - val_accuracy: 0.5804\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 3s 56ms/step - loss: 0.9353 - accuracy: 0.6674 - val_loss: 1.1655 - val_accuracy: 0.5791\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 3s 55ms/step - loss: 0.8947 - accuracy: 0.6828 - val_loss: 1.1748 - val_accuracy: 0.5797\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 3s 56ms/step - loss: 0.8630 - accuracy: 0.6910 - val_loss: 1.2034 - val_accuracy: 0.5935\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 3s 56ms/step - loss: 0.8294 - accuracy: 0.7033 - val_loss: 1.1738 - val_accuracy: 0.5830\n",
      "48/48 [==============================] - 1s 8ms/step\n",
      "Accuracy: 58.30%\n",
      "Recall Score: 58.30\n",
      "Precision: 58.59\n",
      "F1 Score: 58.19\n"
     ]
    }
   ],
   "source": [
    "test_model(\"GRU\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
